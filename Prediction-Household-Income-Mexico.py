# -*- coding: utf-8 -*-
"""proyecto_final(MP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1hGck7draX1is6LBTClgWLwxay9nUsi

# Proyecto Modelado Predictivo


---


## Objetivo de Desarrollo Sostenible No.8 Trabajo Decente y Crecimiento Económico

---

## Integrantes:


*   Arias Morales Yahir
*   https://github.com/Isomorfismo
*   Piña del Valle José
*   Rivera García Axel Maximiliano

## Librerias
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from statsmodels.stats.multicomp import pairwise_tukeyhsd
warnings.filterwarnings("ignore")

pd.options.display.float_format = '{:.5f}'.format

"""## Importación de los datos"""

df = pd.read_csv("concentradohogarproyecto.csv")
df = df.drop(columns="Unnamed: 0")

df = df[df['ingtrab'] != 0]

"""## Entendimiento del problema"""

# Filas y columnas
print("Filas y columnas del DataFrame:")
print(df.shape)

#Variables a utilizar
print("Variables a utilizar:")
print(pd.DataFrame(df.columns))

#Distribución de nuestra variable objetivo, en este caso del ingreso por trabajo
print("Distribución de la variable objetivo (ingreso por trabajo):")
print(df["ingtrab"].describe())

# Informacion del número, tipo y nombre de las variables
df.info()

# Mostrar estadísticas importantes para el análisis de datos
df.describe(include='all')

#Distribución del ingreso por trabajo
plt.figure(figsize=(10, 6))
sns.histplot(df['ingtrab'], bins=30, kde=True)
plt.title('Distribución del Ingreso por Trabajo')
plt.xlabel('Ingreso por Trabajo')
plt.ylabel('Frecuencia')
plt.ticklabel_format(style='plain', axis='x')
plt.grid()
plt.show()

# Correlaciones entre variables numéricas
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
print(correlation_matrix)

"""## Limipieza y preprocesamiento de datos

### Variables numericas
"""

# Seleccionar todas las variables numericas y eliminar ingtrab que es la variable que se busca predecir
df_numericas = df.select_dtypes(include=float)
df_numericas = df_numericas.drop(columns=["ingtrab"])

df_numericas

#Visualizacion de valores atipicos
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_numericas)
plt.title('Boxplot de Variables Numéricas')
plt.xticks(rotation=45)
plt.ticklabel_format(style='plain', axis='y')
plt.grid()
plt.show()

"""### Variables categoricas"""

# Seleccion de las variables de tipo categórico
df_categoricas = df.select_dtypes(include=int)

#histograma de variables categoricas
plt.figure(figsize=(12, 8))
df_categoricas.hist(bins=30, figsize=(15, 10), layout=(3, 3), edgecolor='black')
plt.suptitle('Histogramas de Variables Categóricas')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Limpieza de outliers
def remove_outliers(df, threshold=3):
    for col in df:
        mean = df[col].mean()
        std_dev = df[col].std()
        outliers = (df[col] - mean).abs() > threshold * std_dev
        df = df[~outliers]
    return df

df_cleaned = remove_outliers(df.copy(), threshold=3)

# Visualizacion de las columnas sin outliers
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_cleaned)
plt.title('Boxplot de Variables Numéricas sin Outliers')
plt.xticks(rotation=45)
plt.ticklabel_format(style='plain', axis='y')
plt.grid()
plt.show()

# Contar valores únicos por columna
unique_counts = df.nunique()
print("Valores únicos por columna:")
print(unique_counts)

"""## Transformación logaritmica de los datos"""

#Transformación de las varibles numericas
def transform_numerical(df):
    for col in df.select_dtypes(include=float).columns:
        if df[col].skew() > 1 or df[col].skew() < -1:
            df[col] = np.log1p(df[col])
    return df

#Transformacion Box-Cox
def transform_boxcox(df):
    for col in df.select_dtypes(include=float).columns:
        if df[col].skew() > 1 or df[col].skew() < -1:
            df[col], _ = stats.boxcox(df[col] + 1)
    return df

# df = transform_numerical(df)

#df = transform_boxcox(df)

#Distribucion de todas las variables
plt.figure(figsize=(12, 6))
num_cols = len(df.columns)
num_rows = (num_cols + 2) // 3
for i, col in enumerate(df.columns):
    plt.subplot(num_rows, 3, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(col)
plt.ticklabel_format(style='plain', axis='x')
plt.tight_layout()
plt.show()

"""## Analisis Exploratorio de Datos (EDA)

### Ingreso vs las demas variables

#### Variables numericas
"""

#Ingreso por trabajo vs otras variables numericas

df_numericas = df.select_dtypes(include=float)
df_numericas = df_numericas.drop(columns=["ingtrab"])
for col in df_numericas.columns:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=df[col], y=df['ingtrab'])
    plt.title(f'Ingreso por Trabajo vs {col}')
    plt.xlabel(col)
    plt.ylabel('Ingreso por Trabajo')
    plt.ticklabel_format(style='plain', axis='x')
    plt.grid()
    plt.show()

"""### Variables categoricas"""

# ingtrab vs variables categoricas
df_categoricas = df.select_dtypes(include=int)
for col in df_categoricas.columns:
    plt.figure(figsize=(10, 6))
    sns.barplot(x=df[col], y=df['ingtrab'])
    plt.title(f'Ingreso por Trabajo vs {col}')
    plt.xlabel(col)
    plt.ylabel('Ingreso por Trabajo')
    plt.ticklabel_format(style='plain', axis='y')
    plt.grid()
    plt.show()

"""## Visualización de los Q - Q plots"""

# Gráficas que muestran que tanto los datos se aproximan a una distribución normal estandar
plt.figure(figsize=(10, 20))
for i, col in enumerate(df.columns):
    plt.subplot(num_cols, 3, i+1)
    stats.probplot(df[col], dist="norm", plot=plt)
    plt.title(col)
plt.tight_layout()

"""##### Dado que nuestro enfoque es saber la desigualdad, algunas graficas que podrian ser utilies"""

# Visualizacion de los datos del ingreso de los sexos de los jefes de familia
sns.boxplot(x='sexo_jefe', y='ingtrab', data=df)
df.groupby('sexo_jefe')['ingtrab'].describe().reset_index()

# Visualizacion de los datos del ingreso del hogar agrupado por educacion del jefe
sns.boxplot(x='educa_jefe', y='ingtrab', data=df)
df.groupby('educa_jefe')['ingtrab'].describe().reset_index()

# Visualizacion de los datos del ingreso del hogar agrupado por tamaño de la localidad en la que se encuentra el hogar
sns.boxplot(x='tam_loc', y='ingtrab', data=df)
df.groupby('tam_loc')['ingtrab'].describe().reset_index()

# Visualización de los datos del ingreso del hogar agrupado por estrato socieconomico
sns.boxplot(x='est_socio', y='ingtrab', data=df)
df.groupby('est_socio')['ingtrab'].describe().reset_index()

# visualizacion de los datos agrupado por sexo del jefe
sns.catplot(x='educa_jefe', y='ingtrab', hue='sexo_jefe', kind='box', data=df)

"""## ANOVA"""

#Funcion que realiza el ANOVA de diferentes variables categoricas
def anova_categorical(df, target, categorical_vars):
    results = {}
    for var in categorical_vars:
        model = ols(f'{target} ~ C({var})', data=df).fit()
        anova_table = sm.stats.anova_lm(model, typ=2)
        results[var] = anova_table['PR(>F)'][0]
    return pd.DataFrame(results, index=['p-value']).T

anova_categorical_vars = ['sexo_jefe', 'educa_jefe', 'tam_loc', 'est_socio']
anova_results = anova_categorical(df, 'ingtrab', anova_categorical_vars)
print("Resultados del ANOVA para variables categóricas:")
print(anova_results)

"""En el calulo del ANOVA, dado que los p-value son < 0.05, significa que las variables categoricas son extremadamente significantes con respecto a la variable objetivo que es el ingreso por trabajo."""

def tukey_test(df, target, categorical_var):
    tukey = pairwise_tukeyhsd(endog=df[target], groups=df[categorical_var], alpha=0.05)
    return tukey

"""La prueba Tukey HSD (Honest Significant Difference) nos ayuda a encontrar diferencias significativas entre los diferentes grupos de la variables categoricas, con un 95% de confianza. Asi pues tomamos que el p-value sea < 0.05 para que la **hipotesis nula** (Existe diferencia significativa entre un grupo y otro) se pueda rechazar, en otro caso no hay diferencia significativa entre esos grupos."""

print("Resultados del test de Tukey para 'sexo_jefe':")
print(tukey_test(df, 'ingtrab', 'sexo_jefe'))

"""Primeramente para el sexo del jefe del hogar, la diferencia que hay entre hombres y mujeres con respecto a su ingreso por trabajo es significativa de acuerdo a su p-value, por lo tanto la hipotesis nula se rechaza."""

print("Resultados del test de Tukey para 'educa_jefe':")
print(tukey_test(df, 'ingtrab', 'educa_jefe'))

"""Ahora para la educacion del jefe del hogar, con respecto a su ingreso por trabajo.

- Las personas que tiene como estudio formal el preescolar, no tienen mucha diferencia significativa con el ingreso hasta las personas que ya tienen un estudio profesional completo, las causas pueden ser a que las personas son de una edad adulta con tiempo ya trabajando y que los que tengan estudio profesional sean recien graduados. (Curioso)

- Tampoco las personas que tienen la primaria terminada como las que tienen la secundaria incompleta hay diferencias significativas. Practicamente los dos grupos tienen la misma escolaridad.

- Entre todos los demas grupos si existe una diferencia significativa.
"""

print("Resultados del test de Tukey para 'est_socio':")
print(tukey_test(df, 'ingtrab', 'est_socio'))

"""Para los estratos socioeconomicos tambien existe una diferencia significativa entre los grupos, sobre todo entre los que son de un estrato bajo con los de estrato alto, pero tambien entre los de estrato bajo con los de medio alto."""

print("Resultados del test de Tukey para 'tam_loc':")
print(tukey_test(df, 'ingtrab', 'tam_loc'))

"""En cuestion del tamaño de la localidad si existe diferencia entre los grupos que pertenecen a una localidad con un tamaño en concreto. La prueba nos dice que en localidades de 100,000 habitantes (normalmente grandes ciudades) existe una gran diferencia entre las personas que son de comunidades rurales de alrededor de menos de 2,500 habitantes.

## Correlaciones entre las variables

### Pearson
"""

#Pearson
def pearson_correlation(df, target, numerical_vars):
    results = {}
    for var in numerical_vars:
        corr, p_value = stats.pearsonr(df[target], df[var])
        results[var] = {'correlation': corr, 'p-value': p_value}
    return pd.DataFrame(results).T

numerical_vars = df.select_dtypes(include=float).columns.tolist()
target = 'ingtrab'
pearson_results = pearson_correlation(df, target, numerical_vars)
print("Resultados de la correlación de Pearson:")
print(pearson_results)

"""Las correlaciones nos indican que la varibales trabajo es la que mas se relaciona con la de ingreso por trabajo, aunque tambien el gasto monetario que se hace en los hogares esta relacionado con el ingreso. Como en anteriores graficos se cumple que mientras mas se ingresa mas es la capacidad de gasto.

### Spearman
"""

#Spearman
def spearman_correlation(df, target, numerical_vars):
    results = {}
    for var in numerical_vars:
        corr, p_value = stats.spearmanr(df[target], df[var])
        results[var] = {'correlation': corr, 'p-value': p_value}
    return pd.DataFrame(results).T

spearman_results = spearman_correlation(df, target, numerical_vars)
print("Resultados de la correlación de Spearman:")
print(spearman_results)

"""Ahora con la correlación de Spearman, podemos decir que nuestras variables no tienen del todo un comportamiento lineal sino no lineal. Lo que nos puede intuir que se necesita un modelo como la regresión polinomial o algun tipo de regresor especial, sea un árbol u otro modelo.

## Construcción del modelo

### Modelos seleccionados: Regresion lineal multiple, Polinomica, Arbol de Decisión, Bosque aleatorio
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline

"""### División del conjunto de datos"""

X = df.drop(columns=['ingtrab'])
y = df['ingtrab']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)

"""### Regresión lineal"""

linear_model = LinearRegression()
resultados = cross_val_score(linear_model, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo lineal:")
print(resultados)

"""### Regresion polinomica"""

pipeline = Pipeline([
    ('poly_features', PolynomialFeatures(degree=2)),
    ('linear_regression', LinearRegression())
])

resultados_poly = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2').mean()
print("Resultados de la validación cruzada del modelo polinómico:")
print(resultados_poly)

"""### Árbol de decisión"""

tree = DecisionTreeRegressor(random_state=0)
resultados_tree = cross_val_score(tree, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo de árbol de decisión:")
print(resultados_tree)

"""### Bosque Aleatorio"""

bosque = RandomForestRegressor(random_state=0)
resultados_bosque = cross_val_score(bosque, X_train, y_train, cv=5, scoring='r2').mean()

print("Resultados de la validación cruzada del modelo de bosque aleatorio:")
print(resultados_bosque)

"""## Modelo final

#### Finalmente la regresión polinomica fue la que tuvo mejor desempeño, aunque por cuestión de recursos tambien el bosque aleatorio se podria tomar en cuenta.

#### Regresión polinomica
"""

pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Resultados del modelo polinómico:")
print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

"""### Ecuación de nuestro modelo de regresión polinomica"""

variables = pipeline.named_steps['poly_features'].get_feature_names_out(X.columns)

# Obtener coeficientes
coefs = pipeline.named_steps['linear_regression'].coef_
intercept = pipeline.named_steps['linear_regression'].intercept_

#Transformar los coeficientes de vuelta al espacio original
coefs = np.exp(coefs) - 1
intercept = np.exp(intercept) - 1

# Armar la ecuación en formato texto
ecuacion = f"y = {intercept:.3f}"
for coef, nombre in zip(coefs, variables):
    sign = " + " if coef >= 0 else " - "
    ecuacion += f"{sign}{abs(coef):.3f}*{nombre}"

print(ecuacion)

"""### Bosque aleatorio"""

bosque.fit(X_train, y_train)
y_pred_bosque = bosque.predict(X_test)

mse_bosque = mean_squared_error(y_test, y_pred_bosque)
r2_bosque = r2_score(y_test, y_pred_bosque)

print("Resultados del modelo de bosque aleatorio:")
print(f"Mean Squared Error: {mse_bosque}")
print(f"R^2 Score: {r2_bosque}")

"""#### Visualización del árbol, de modo que veamos como el bosque funciono"""

#Visualizacion de un arbol del bosque aleatorio
from sklearn.tree import plot_tree

plt.figure(figsize=(20,10))
plot_tree(bosque.estimators_[0], feature_names=X.columns, max_depth=3, filled=True)
plt.show()

"""#### Visualización del bosque"""

#Visualizacion del bosque aleatorio
plt.figure(figsize=(12, 8))
importances = bosque.feature_importances_
indices = np.argsort(importances)[::-1]
plt.bar(range(X_train.shape[1]), importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.title('Importancia de las Características del Bosque Aleatorio')
plt.xlabel('Características')
plt.ylabel('Importancia')
plt.tight_layout()
plt.show()

"""### Conclusiones

Este proyecto logró integrar exitosamente el análisis exploratorio de datos y la predicción del ingreso por sueldo de los hogares en México, utilizando datos de la Encuesta Nacional de Ingresos y Gastos de los Hogares (ENIGH). A través de una plataforma interactiva desarrollada con Streamlit, se ofreció una experiencia visual y dinámica que permite comprender los factores que inciden en el ingreso laboral.

Durante el análisis exploratorio, se aplicaron herramientas estadísticas y gráficas que revelaron patrones importantes: por ejemplo, se observó que el nivel educativo del jefe de hogar y el estatus socioeconómico tienen una relación directa con los ingresos, mientras que el tamaño de la localidad mostró una relación inversa, probablemente debido a mayores costos de vida en zonas urbanas. Además, se identificaron variables con baja correlación, así como valores atípicos que fueron tratados adecuadamente para evitar sesgos.

El modelo predictivo fue construido mediante una fórmula ajustada basada en regresión polinomial, que toma en cuenta múltiples interacciones entre variables como sueldos, educación, gasto, horas extra, sexo del jefe de hogar y otras. Este enfoque permitió generar estimaciones personalizadas del ingreso laboral con base en las características socioeconómicas del hogar. La funcionalidad para guardar múltiples predicciones y compararlas gráficamente ofrece una herramienta poderosa para el análisis comparativo y la simulación de escenarios.

La plataforma final permite no solo visualizar y analizar los datos originales, sino también explorar cómo cambios en diferentes variables afectan el ingreso predicho. Esta herramienta tiene potencial para ser utilizada en contextos educativos, gubernamentales y de análisis social, promoviendo la toma de decisiones informadas basadas en datos reales y modelado estadístico.

En conclusión, este proyecto demuestra cómo la integración de ciencia de datos, visualización interactiva y modelado predictivo puede aportar valor en la comprensión de fenómenos económicos complejos como el ingreso laboral en México.
"""
